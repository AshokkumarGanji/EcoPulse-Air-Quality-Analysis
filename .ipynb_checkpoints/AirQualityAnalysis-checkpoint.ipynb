{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "987aa9d6-3d02-44ff-8590-4c17cf9417fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stn_code: string (nullable = true)\n",
      " |-- sampling_date: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- agency: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- so2: string (nullable = true)\n",
      " |-- no2: string (nullable = true)\n",
      " |-- rspm: string (nullable = true)\n",
      " |-- spm: string (nullable = true)\n",
      " |-- location_monitoring_station: string (nullable = true)\n",
      " |-- pm2_5: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n",
      "+--------+------------------+--------------+---------+------+--------------------+---+----+----+---+---------------------------+-----+----------+\n",
      "|stn_code|     sampling_date|         state| location|agency|                type|so2| no2|rspm|spm|location_monitoring_station|pm2_5|      date|\n",
      "+--------+------------------+--------------+---------+------+--------------------+---+----+----+---+---------------------------+-----+----------+\n",
      "|     150|February - M021990|Andhra Pradesh|Hyderabad|    NA|Residential, Rura...|4.8|17.4|  NA| NA|                         NA|   NA|1990-02-01|\n",
      "|     151|February - M021990|Andhra Pradesh|Hyderabad|    NA|     Industrial Area|3.1|   7|  NA| NA|                         NA|   NA|1990-02-01|\n",
      "|     152|February - M021990|Andhra Pradesh|Hyderabad|    NA|Residential, Rura...|6.2|28.5|  NA| NA|                         NA|   NA|1990-02-01|\n",
      "|     150|   March - M031990|Andhra Pradesh|Hyderabad|    NA|Residential, Rura...|6.3|14.7|  NA| NA|                         NA|   NA|1990-03-01|\n",
      "|     151|   March - M031990|Andhra Pradesh|Hyderabad|    NA|     Industrial Area|4.7| 7.5|  NA| NA|                         NA|   NA|1990-03-01|\n",
      "+--------+------------------+--------------+---------+------+--------------------+---+----+----+---+---------------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize PySpark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Air Pollution Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the schema to check the columns\n",
    "df.printSchema()\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef34298-9dd1-406e-b5d6-8694c5aa8ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+--------+------+----+---+---+----+---+---------------------------+-----+----+\n",
      "|stn_code|sampling_date|state|location|agency|type|so2|no2|rspm|spm|location_monitoring_station|pm2_5|date|\n",
      "+--------+-------------+-----+--------+------+----+---+---+----+---+---------------------------+-----+----+\n",
      "|       0|            0|    0|       0|     0|   0|  0|  0|   0|  0|                          0|    0|   0|\n",
      "+--------+-------------+-----+--------+------+----+---+---+----+---+---------------------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "null_counts = df.select([spark_sum(col(column).isNull().cast(\"int\")).alias(column) for column in df.columns])\n",
    "\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6428042c-99e7-406e-81d2-204ce35b8c96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'stn_code': 144077 'NA' values\n",
      "Column 'sampling_date': 3 'NA' values\n",
      "Column 'state': 0 'NA' values\n",
      "Column 'location': 3 'NA' values\n",
      "Column 'agency': 149481 'NA' values\n",
      "Column 'type': 5393 'NA' values\n",
      "Column 'so2': 34646 'NA' values\n",
      "Column 'no2': 16233 'NA' values\n",
      "Column 'rspm': 40222 'NA' values\n",
      "Column 'spm': 237387 'NA' values\n",
      "Column 'location_monitoring_station': 27491 'NA' values\n",
      "Column 'pm2_5': 426428 'NA' values\n",
      "Column 'date': 7 'NA' values\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize a list to store the counts of 'NA' values in each column\n",
    "na_counts = []\n",
    "\n",
    "# Iterate over each column in the DataFrame\n",
    "for column in df.columns:\n",
    "    # Count the number of 'NA' values in the current column\n",
    "    na_count = df.filter(col(column).isNull() | (col(column) == \"NA\")).count()\n",
    "    # Append the count to the list\n",
    "    na_counts.append((column, na_count))\n",
    "\n",
    "# Print the count of 'NA' values in each column\n",
    "for column, count in na_counts:\n",
    "    print(f\"Column '{column}': {count} 'NA' values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4657816c-908a-4380-b5c7-c47def99055c",
   "metadata": {},
   "source": [
    "# Analyzing Average SO2 Concentration Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df9eec9b-72cb-48d1-88bd-e8b4fbb31f03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|year|          avg(so2)|\n",
      "+----+------------------+\n",
      "|NULL|               9.5|\n",
      "|1987|21.827070056443002|\n",
      "|1988|22.098102472736667|\n",
      "|1989| 18.66806618365016|\n",
      "|1990|17.723146864250822|\n",
      "|1991|17.607666881492772|\n",
      "|1992| 17.71976534345789|\n",
      "|1993| 21.95889211841384|\n",
      "|1994| 22.47885135718294|\n",
      "|1995| 26.00087719669281|\n",
      "|1996|20.645564293943522|\n",
      "|1997|21.279954207585302|\n",
      "|1998|20.031751628797096|\n",
      "|1999|20.288985503756482|\n",
      "|2000|16.913942321020567|\n",
      "|2001|15.250710649175693|\n",
      "|2002|13.151781481178809|\n",
      "|2003| 6.557118499573742|\n",
      "|2004|11.534256824637112|\n",
      "|2005|12.829258540113173|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, col\n",
    "\n",
    "# Extract year from the 'date' column\n",
    "df_with_year = df.withColumn(\"year\", year(col(\"date\")))\n",
    "\n",
    "# Convert 'so2' column to numerical type\n",
    "df_with_year = df_with_year.withColumn(\"so2\", col(\"so2\").cast(\"float\"))\n",
    "\n",
    "# Group by year and calculate the average SO2 concentration\n",
    "avg_so2_per_year = df_with_year.groupBy(\"year\").agg({\"so2\": \"avg\"})\n",
    "\n",
    "# Show the average SO2 concentration in each year\n",
    "# avg_so2_per_year.show()\n",
    "\n",
    "# Sort the DataFrame by year in ascending order\n",
    "sorted_avg_so2_per_year = avg_so2_per_year.orderBy(\"year\")\n",
    "\n",
    "# Show the sorted DataFrame\n",
    "sorted_avg_so2_per_year.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d444cce-a5bd-4d97-9916-93c587ba8ab1",
   "metadata": {},
   "source": [
    "# Analyzing Average NO2 Concentration Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45fc2a3e-1e82-4116-a527-18214127540e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|year|          avg(no2)|\n",
      "+----+------------------+\n",
      "|NULL|              22.0|\n",
      "|1987|30.584545396674763|\n",
      "|1988|30.606439402496274|\n",
      "|1989|29.215815630826082|\n",
      "|1990|25.714481812678713|\n",
      "|1991|25.986804675469887|\n",
      "|1992| 30.70205831471479|\n",
      "|1993| 30.22762870702128|\n",
      "|1994|31.764828516078943|\n",
      "|1995| 33.07806643906677|\n",
      "|1996| 26.50331755162949|\n",
      "|1997|28.093304541661983|\n",
      "|1998|27.252314541237485|\n",
      "|1999|28.404222968663717|\n",
      "|2000|29.006001061793025|\n",
      "|2001| 28.69994952712362|\n",
      "|2002|27.131232261431727|\n",
      "|2003| 21.49914821124361|\n",
      "|2004|  28.0458437499995|\n",
      "|2005|28.500494903343327|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, col\n",
    "\n",
    "# Convert 'no2' column to numerical type\n",
    "df_with_year_no2 = df.withColumn(\"year\", year(col(\"date\"))) \\\n",
    "                      .withColumn(\"no2\", col(\"no2\").cast(\"float\"))\n",
    "\n",
    "# Group by year and calculate the average NO2 concentration\n",
    "avg_no2_per_year = df_with_year_no2.groupBy(\"year\").agg({\"no2\": \"avg\"})\n",
    "\n",
    "# Sort the DataFrame by year in ascending order\n",
    "sorted_avg_no2_per_year = avg_no2_per_year.orderBy(\"year\")\n",
    "\n",
    "# Show the sorted DataFrame\n",
    "sorted_avg_no2_per_year.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe7166-bab8-4243-91c4-0784bf4ba525",
   "metadata": {},
   "source": [
    "# Analyzing Average SO2 Concentration Across Indian States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfc82280-619f-46b0-8a02-c82414364cf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|               state|          avg(so2)|\n",
      "+--------------------+------------------+\n",
      "|         Uttaranchal| 24.69773585481464|\n",
      "|         Uttarakhand|24.372957176868667|\n",
      "|           Jharkhand|23.485793502432667|\n",
      "|              Sikkim|19.799999237060547|\n",
      "|               Bihar|19.381476262504638|\n",
      "|         Maharashtra|17.366863001066815|\n",
      "|             Gujarat|16.656342665415362|\n",
      "|             Haryana| 14.06495699315893|\n",
      "|        Chhattisgarh|12.846609351416697|\n",
      "|         West Bengal|12.608765600450694|\n",
      "|       Uttar Pradesh|12.528499862668342|\n",
      "|          Puducherry|11.970638741482741|\n",
      "|      Madhya Pradesh|11.587409813909497|\n",
      "|          Tamil Nadu|11.315134176057505|\n",
      "|              Punjab| 10.62859814358437|\n",
      "|           Karnataka|10.223098793866434|\n",
      "|           Meghalaya| 8.955907852839397|\n",
      "|Dadra & Nagar Haveli| 8.939586647743262|\n",
      "|               Delhi| 8.737272731366444|\n",
      "|         Daman & Diu| 8.192957754782311|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert 'so2' column to numerical type\n",
    "df_with_state_so2 = df.withColumn(\"so2\", col(\"so2\").cast(\"float\"))\n",
    "\n",
    "# Group by state and calculate the average SO2 concentration\n",
    "avg_so2_per_state = df_with_state_so2.groupBy(\"state\").agg({\"so2\": \"avg\"})\n",
    "\n",
    "# Sort the DataFrame by average SO2 concentration in descending order\n",
    "sorted_avg_so2_per_state = avg_so2_per_state.orderBy(col(\"avg(so2)\").desc())\n",
    "\n",
    "# Show the sorted DataFrame\n",
    "sorted_avg_so2_per_state.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5797bad4-aea4-4999-8a84-5b07137bb6ca",
   "metadata": {},
   "source": [
    "# Analyzing Average NO2 Concentration Across Indian States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37241ec4-3c5a-4368-9992-5104c3236ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|         state|          avg(no2)|\n",
      "+--------------+------------------+\n",
      "|   West Bengal|59.075731456493465|\n",
      "|         Delhi| 53.48914699109586|\n",
      "|     Jharkhand| 43.36634063622639|\n",
      "|         Bihar| 36.57552511180373|\n",
      "|   Maharashtra| 32.11537044823239|\n",
      "|     Rajasthan|30.441008210725574|\n",
      "|        Punjab| 28.08584599447385|\n",
      "| Uttar Pradesh|27.610095250614034|\n",
      "|   Uttaranchal|27.163018903192484|\n",
      "|   Uttarakhand| 26.93809002978461|\n",
      "|        Sikkim|26.799999237060547|\n",
      "|  Chhattisgarh|24.815961301764144|\n",
      "|       Gujarat|24.065630833599833|\n",
      "|     Telangana| 23.86400508469653|\n",
      "|       Haryana|23.428310646157783|\n",
      "|     Karnataka|22.702836508411135|\n",
      "|Andhra Pradesh| 21.70445055814219|\n",
      "|    Tamil Nadu| 21.60120197385065|\n",
      "|       Manipur|20.173684182919953|\n",
      "|Madhya Pradesh|18.639596246818286|\n",
      "+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert 'no2' column to numerical type\n",
    "df_with_state_no2 = df.withColumn(\"no2\", col(\"no2\").cast(\"float\"))\n",
    "\n",
    "# Group by state and calculate the average NO2 concentration\n",
    "avg_no2_per_state = df_with_state_no2.groupBy(\"state\").agg({\"no2\": \"avg\"})\n",
    "\n",
    "# Sort the DataFrame by average NO2 concentration in descending order\n",
    "sorted_avg_no2_per_state = avg_no2_per_state.orderBy(col(\"avg(no2)\").desc())\n",
    "\n",
    "# Show the sorted DataFrame\n",
    "sorted_avg_no2_per_state.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35feb31-c93d-466e-bda4-0ee303f7b674",
   "metadata": {},
   "source": [
    "# Analyzing Average SO2 Concentration by Pollution Source Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e884124-3e13-4555-9120-db2e464c74fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                type|          avg(so2)|\n",
      "+--------------------+------------------+\n",
      "|    Industrial Areas|13.765722947674599|\n",
      "|         Residential|15.227777778550431|\n",
      "|     Industrial Area|13.212287439513934|\n",
      "|     Sensitive Areas| 7.477117815930051|\n",
      "|          Industrial|27.130601067360633|\n",
      "|Residential, Rura...| 9.218446749960986|\n",
      "|Residential and o...|10.246096335867149|\n",
      "|      Sensitive Area| 5.263544508812119|\n",
      "|               RIRUO|10.912576687116564|\n",
      "|           Sensitive|10.486184225303301|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert 'so2' column to numerical type\n",
    "df_with_type_so2 = df.withColumn(\"so2\", col(\"so2\").cast(\"float\"))\n",
    "\n",
    "# Filter out rows with 'NA' values in the 'type' column\n",
    "df_filtered = df_with_type_so2.filter(df_with_type_so2[\"type\"] != \"NA\")\n",
    "\n",
    "# Group by type and calculate the average SO2 concentration\n",
    "avg_so2_per_type = df_filtered.groupBy(\"type\").agg({\"so2\": \"avg\"})\n",
    "\n",
    "# Show the DataFrame\n",
    "avg_so2_per_type.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd401e25-d220-4e51-8a2f-b58e82068113",
   "metadata": {},
   "source": [
    "# Analyzing Average NO2 Concentration by Pollution Source Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c0fb752-6297-4aca-8e37-f3278fe6d500",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                type|          avg(no2)|\n",
      "+--------------------+------------------+\n",
      "|    Industrial Areas| 29.99172684025098|\n",
      "|         Residential|  19.4928571736174|\n",
      "|     Industrial Area|  29.1711924586431|\n",
      "|     Sensitive Areas|22.871587705858854|\n",
      "|          Industrial|37.864766704294965|\n",
      "|Residential, Rura...|23.410190313693487|\n",
      "|Residential and o...| 25.38933156449053|\n",
      "|      Sensitive Area| 18.06023982313901|\n",
      "|               RIRUO|31.779907975460123|\n",
      "|           Sensitive| 24.61872341074842|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert 'no2' column to numerical type\n",
    "df_with_type_no2 = df.withColumn(\"no2\", col(\"no2\").cast(\"float\"))\n",
    "\n",
    "# Filter out rows with 'NA' values in the 'type' column\n",
    "df_filtered_no2 = df_with_type_no2.filter(df_with_type_no2[\"type\"] != \"NA\")\n",
    "\n",
    "# Group by type and calculate the average NO2 concentration\n",
    "avg_no2_per_type = df_filtered_no2.groupBy(\"type\").agg({\"no2\": \"avg\"})\n",
    "\n",
    "# Show the DataFrame\n",
    "avg_no2_per_type.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c32186-2b7e-4d24-8649-0066d851de4c",
   "metadata": {},
   "source": [
    "# # Analyzing Average pm2_5 Concentration Across Indian States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ef6e0b9-f321-40ef-b67b-284446b8a288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|               state|        avg(pm2_5)|\n",
      "+--------------------+------------------+\n",
      "|         Daman & Diu|27.886363636363637|\n",
      "|                 Goa| 18.85561151024249|\n",
      "|             Gujarat| 30.72969596001666|\n",
      "|               Delhi| 95.11320754716981|\n",
      "|Dadra & Nagar Haveli|30.511627906976745|\n",
      "|      Madhya Pradesh| 65.06456451508046|\n",
      "|              Odisha| 42.20408895265423|\n",
      "|          Tamil Nadu|29.550440534095934|\n",
      "|           Telangana| 43.96892655367232|\n",
      "|         West Bengal|         64.890625|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert 'pm2_5' column to numerical type\n",
    "df_with_state_pm25 = df.withColumn(\"pm2_5\", col(\"pm2_5\").cast(\"float\"))\n",
    "\n",
    "# Group by state and calculate the average PM2.5 concentration\n",
    "avg_pm25_per_state = df_with_state_pm25.groupBy(\"state\").agg({\"pm2_5\": \"avg\"})\n",
    "\n",
    "# Filter out rows with non-null average PM2.5 values\n",
    "avg_pm25_per_state_filtered = avg_pm25_per_state.filter(col(\"avg(pm2_5)\").isNotNull())\n",
    "\n",
    "# Show the DataFrame\n",
    "avg_pm25_per_state_filtered.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f508df1e-8685-4b30-84f1-e0f2a4731b08",
   "metadata": {},
   "source": [
    "# Analyzing Average NO2 Concentration by Pollution Source Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b9adff9-e9e3-4607-8e35-871ba44e8837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|                type|       avg(pm2_5)|\n",
      "+--------------------+-----------------+\n",
      "|     Industrial Area|44.10698739933394|\n",
      "|Residential, Rura...|39.65274991596318|\n",
      "|               RIRUO|37.81629116117851|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert 'pm2_5' column to numerical type\n",
    "df_with_type_pm25 = df.withColumn(\"pm2_5\", col(\"pm2_5\").cast(\"float\"))\n",
    "\n",
    "# Group by type and calculate the average PM2.5 concentration\n",
    "avg_pm25_per_type = df_with_type_pm25.groupBy(\"type\").agg({\"pm2_5\": \"avg\"})\n",
    "\n",
    "# Filter out rows with non-null average PM2.5 values\n",
    "avg_pm25_per_type_filtered = avg_pm25_per_type.filter(col(\"avg(pm2_5)\").isNotNull())\n",
    "\n",
    "# Show the DataFrame\n",
    "avg_pm25_per_type_filtered.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16816b-8bfe-46a9-8337-6024a79a8ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
